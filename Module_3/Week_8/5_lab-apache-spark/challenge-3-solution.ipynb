{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3: PySpark Demo\n",
    "\n",
    "In this Challenge we'll show you how to use Spark to run intensive computation jobs in Jupyter Notebook. So far you should have installed PySpark in local mode, configured your environment variables, and started Jupyter Notebook with PySpark. If you haven't completed any of the previous steps, please go back and complete them because otherwise you won't be able to complete this challenge.\n",
    "\n",
    "If everything works so far, you should be able to create a `SparkContext` instance in Jupyter Notebook. Try to execute the next cell where the code is already written for you to create a new `SparkContext` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a `SparkContext` instance or reuse an existing one\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No errors? Congrats! But in case you do, review Challenge 2 and google your error to see what could have possibly gone wrong.\n",
    "\n",
    "SparkContext is a library that helps you access the Spark execution environment in order to use the [Spark Python APIs](https://spark.apache.org/docs/latest/api/python/index.html). We will show you an example of how to call the Spark API using the SparkContext instance.\n",
    "\n",
    "But before we start the real work, we'd like you to get familiar with the concept of [benchmarking](https://en.wikipedia.org/wiki/Benchmark_(computing)). Benchmarking is a common technique in software engineering and data engineering to evaluate the efficiency of your codes. The idea is to measure the execution time of your program and research if there is room to improve your code in order to get the job done with shorter time. In machine learning a lot of times you'be doing repeated sampling and complex queries that could run for hours, days, or even weeks. It is important that you improve your code efficiency whenever you can. Otherwise you'll waste a lot of time and computation powers in executing inefficient code.\n",
    "\n",
    "We will benchmark a code snippet that estimates Pi (`π`) by repeated sampling. We will first benchmark this without using Spark. The idea of the Pi estimation is to randomly generate many points with `x` and `y` coordinates between 0 and 1. These points will fall in a square whose side length is 1 (the upper right grid of the square in the image below that combines the red and green areas). We will count the number of sample points that fall into the 1/4 circle sector (the area in red) against all points which allows us to calculate Pi.\n",
    "\n",
    "![pi.png](pi.png)\n",
    "\n",
    "Below are the math formulas to calculate the probability of a point to fall into the 1/4 circle sector (*A0*) and the square area (*A0*).\n",
    "\n",
    "```\n",
    "A0 = π * r^2\n",
    "A1 = (2r)^2\n",
    "```\n",
    "\n",
    "From the formulas above, you can deduce ` π = 4 * A0` if *r* is 1, which means Pi equals to 4 times the probability of a point falling into the red area.\n",
    "\n",
    "Because this lab focuses on Spark, we will provide you the code snippet for estimating Pi based on the mathematical concept discussed above. Read the code carefully and make sure you fully understand what the code does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def inside(p):\n",
    "    \"\"\"\n",
    "    Generate a random point and check if the point is within the circle with radius=1.\n",
    "    \n",
    "    Returns:\n",
    "        (bool) whether the generated point is within the circle area\n",
    "    \"\"\"\n",
    "    x, y = np.random.random(), np.random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "def estimate_pi(num_samples):\n",
    "    \"\"\"\n",
    "    Estimate the value of Pi by means of repeated sampling. Benchmark the repeated sampling time cost.\n",
    "    \n",
    "    Params:\n",
    "        num_sampes (int): the number of sample points to generate\n",
    "    \n",
    "    Returns:\n",
    "        (float) estimated value of Pi\n",
    "    \"\"\"\n",
    "    print(\"Executing Spark job...\")\n",
    "    start = timer()\n",
    "    dots = list(filter(inside,list(range(num_samples)))) \n",
    "    count = len(dots)\n",
    "    end = timer()\n",
    "    print(\"Spark job ended. Total time elapsed: %s\" % (end-start))\n",
    "    return(4.0 * count / num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, call `estimate_pi()` with `50000`, `500000`, and `5000000` sample sizes. See what you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Spark job...\n",
      "Spark job ended. Total time elapsed: 4.821447065999999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.1405456"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "estimate_pi(5000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the excution time increases when you increase the sample size. In addition, the accuracy of the Pi value increases as you increase the sample size. But if you keep increasing the sample size you have to wait a long time for the script to finish execution. What can you do if you really need to run a very large sample size?\n",
    "\n",
    "There are several options. You can **use a computer with higher processing speed**. This allows you to execute the sampling jobs faster. But this is usually not the best option because the improvement of CPU processing speed has limited impacts on executing millions of sampling jobs.\n",
    "\n",
    "The second option is to **run multiple sampling jobs in parallel**. In case you don't know yet, Python is a linear programming language which means it executes one job then the next. If you wait for millions of samping jobs to finish in a single queue it will take you a lot of time. Therefore, most modern programming languages including Python and R have introduced a feature called [parallel programming or parallel processing](https://wiki.python.org/moin/ParallelProcessing) so that you can save time by running several tasks at the same time. Say if your sampling jobs would take 1,000 seconds to finish in a single queue, you can finish them in 250 seconds with 4 parallel processing queues. Note that this time calculation is theoretical. In reality with 4 parallel queues the finishing time is usually more than 250 seconds because Python has to spend some overhead time to [spawn](https://en.wikipedia.org/wiki/Spawn_(computing)) the parallel processes.\n",
    "\n",
    "**How many parallel processes you can run depends on how many CPU cores you have** on your computer. If you have 8 CPU cores on your computer, the max number of parallel processes you can run is 8. What if you attempt to run more processes than the number of your CPU cores? The excessive jobs will be queued up in the same CPU core which makes your parallel programming meaningless. Therefore, in order to run parallel programming you need to use a multi-core computer and the parallel processes can't exceed your number of CPU cores. \n",
    "\n",
    "Then you may ask, **should I allocate all my CPU cores to parallel programming?** Remember that besides executing the parallel programming scripts, your computer's CPUs also need to run your operation system and some other system processes. Therefore, **you typically reserve 1-2 CPU cores for the OS and system processes and allocate the rest to parallel programming**. However, with Apache Spark you don't need to manually allocate the CPU cores because Spark will automatically do that for you depending on how many cores it has access to. Remember in Challenge 2 we asked you to use the following command to start Spark:\n",
    "\n",
    "```\n",
    "$SPARK_PATH/bin/pyspark --master \"local[*]\"\n",
    "```\n",
    "\n",
    "The `*` directive tells Sparks to decide how many CPU cores to use by itself. If you start Spark with `local[2]`, it means you allocate 2 CPU cores to Spark.\n",
    "\n",
    "Another note is **your computer memory may also set a bottleneck on the performance of parallel programming**. When your computer runs parallel jobs, its memory consumption is multiplied. If a single process consumes 256mb memory, four parallel processes will consume 1,024mb memory. This is in addition to the memory consumption of Apache Spark that keeps your dataset in the memory to improve processing speed. Therefore, in order to use Spark successfully in local mode, you really need a high-end computer with multi-cores and high memory (8gb or above). If your computer runs out of memory, your parallel scripts will be shut down.  \n",
    "\n",
    "So far we have discussed the multi-core and memory concerns of running Spark in the local mode. These concerns are similar if you run Spark in the cluster mode. With the cluster mode each cluster has its own memory and one or multiple cores. Spark is smart enough to automatically allocate the resources in the cluster mode too.\n",
    "\n",
    "Enough has been explained. Now let's move on to the real stuff. How do you run parallel programming with Apache Spark? **That is achieved by involing Spark's `parallelize` method** ([documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html#parallelized-collections)). In the next cell, we have a function called `estimate_pi_parallel` for you to complete. You will rewrite the `estimate_pi` function we gave you earlier and use `SparkContext.parallelize` to spawn parallel processes to create the random point samples then use the samples to estimate Pi. \n",
    "\n",
    "If you are stuck, you can reference [this Jupyter Notebook example](https://github.com/mGalarnyk/Installations_Mac_Ubuntu_Windows/blob/master/Spark/Estimating%20PI.ipynb) (but don't simply copy and paste). **Make sure to benchmark your function so that we know if Spark helps improve the code executing time.**\n",
    "\n",
    "*Hint: You can re-use the `inside` function we gave you earlier in your code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pi_parallel(num_samples):\n",
    "    \"\"\"\n",
    "    Estimate the value of Pi by means of repeated sampling using Spark's `parallelize` method. \n",
    "    Benchmark the repeated sampling time cost.\n",
    "    \n",
    "    Params:\n",
    "        num_sampes (int): the number of sample points to generate\n",
    "    \n",
    "    Returns:\n",
    "        (float) estimated value of Pi\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    #return None\n",
    "    print(\"Executing Spark job...\")\n",
    "    start = timer()\n",
    "    dots = sc.parallelize(range(num_samples)).filter(inside)\n",
    "    count = dots.count()\n",
    "    end = timer()\n",
    "    print(\"Spark job ended. Total time elapsed: %s\" % (end-start))\n",
    "    return(4.0 * count / num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, test your `estimate_pi_parallel` function with `5000000` and `50000000` sample sizes. Run it more than once for each sample size to see how the execution time varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Spark job...\n",
      "Spark job ended. Total time elapsed: 1.5420191240000065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.1425792"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "estimate_pi_parallel(5000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have noticed the execution time is significantly shorter with Spark's `parallelize` method than without using Spark. However, if you find the execution time is actually longer, it could be because your code is not efficient or Spark doesn't have access to multiple CPU cores. If it's the latter case, don't worry. You can still experience the power of parallel programming in Challenge 4 when we run Spark in the cluster mode.\n",
    "\n",
    "You may have also noticed the first time to execute your function takes significantly longer time than executing it in the second and third time (you need to execute the commands consequtively in a row). This is because at the first time you execute the function, Spark has to acquire the memory and CPUs it needs from the system. After the execution is finished Spark does not release the memory immediately. So when you execute the script consequtively, Spark does not need to acquire the hardware resources again.\n",
    "\n",
    "Spark is written in Java (you are interacting with its Java core via a Python wrapper). Java applications typically maintain a minimum memory even if it is idling. When memory-demanding processes are being executed, Java will *burst* its memory by acquiring more from the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling with Pyspark Dataframes\n",
    "\n",
    "Next, you will be practicing data wrangling with the Pandas-like dataframes that Pyspark provides you. You should have already used the equivalent of these functions with Pandas in the previous work. We just want to show you the Spark way of doing the similar things. Note that the syntax in Pyspark is different from that in Pandas. Use Google to find the examples you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import `SparkSession` from `pyspark.sql`. Create a new instance of SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('patients').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read a CSV dataset from a previous lab using `spark.read.csv()` and assign the returned dataframe into a variable called `patients`.\n",
    "\n",
    "The dataset path is `../lab-sklearn-and-unsupervised-learning/patient-admission-dataset-for-learning-data-mining.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "patients = spark.read.csv('../lab-sklearn-and-unsupervised-learning/patient-admission-dataset-for-learning-data-mining.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print a summary of `patients` using `printSchema()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- patient_name: string (nullable = true)\n",
      " |-- patient_email: string (nullable = true)\n",
      " |-- doctor_phone: string (nullable = true)\n",
      " |-- patient_gender: string (nullable = true)\n",
      " |-- patient_dob: string (nullable = true)\n",
      " |-- patient_diabetic: boolean (nullable = true)\n",
      " |-- patient_allergic: boolean (nullable = true)\n",
      " |-- patient_weight_kg: integer (nullable = true)\n",
      " |-- patient_height_sm: integer (nullable = true)\n",
      " |-- patient_nhs_number: long (nullable = true)\n",
      " |-- doctor_name: string (nullable = true)\n",
      " |-- appointment_date: string (nullable = true)\n",
      " |-- patient_show: boolean (nullable = true)\n",
      " |-- is_regular_visit: boolean (nullable = true)\n",
      " |-- prescribed_medicines: string (nullable = true)\n",
      " |-- diagnosis: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "patients.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the missing values in the `patients[\"diagnosis\"]`. Your output should be `488`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "488"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "from pyspark.sql.functions import isnan\n",
    "patients.filter((patients[\"diagnosis\"] == \"\") | patients[\"diagnosis\"].isNull() | isnan(patients[\"diagnosis\"])).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the `diagnosis` column in `patients`.\n",
    "\n",
    "*Hint: First select the column then use the `show()` method*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|diagnosis|\n",
      "+---------+\n",
      "|     I669|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|  S72309N|\n",
      "|     null|\n",
      "|   T508X6|\n",
      "|     null|\n",
      "|     S420|\n",
      "|    T8743|\n",
      "|  M80072A|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "|  T22342A|\n",
      "|     null|\n",
      "|     null|\n",
      "|     null|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "patients.select('diagnosis').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill the missing values in column `diagnosis` with a string `no diagnosis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "patients = patients.fillna('no diagnosis', subset=['diagnosis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the `diagnosis` column again to confirm null values are replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   diagnosis|\n",
      "+------------+\n",
      "|        I669|\n",
      "|no diagnosis|\n",
      "|no diagnosis|\n",
      "|no diagnosis|\n",
      "|no diagnosis|\n",
      "|no diagnosis|\n",
      "|     S72309N|\n",
      "|no diagnosis|\n",
      "|      T508X6|\n",
      "|no diagnosis|\n",
      "|        S420|\n",
      "|       T8743|\n",
      "|     M80072A|\n",
      "|no diagnosis|\n",
      "|no diagnosis|\n",
      "|no diagnosis|\n",
      "|     T22342A|\n",
      "|no diagnosis|\n",
      "|no diagnosis|\n",
      "|no diagnosis|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "patients.select('diagnosis').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the missing values in column `doctor_name`. You should see output `58`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "patients.filter((patients[\"doctor_name\"] == \"\") | patients[\"doctor_name\"].isNull() | isnan(patients[\"doctor_name\"])).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop the rows in `patients` where column `doctor_name` is missing data. Remember to assign the converted dataset back to `patients`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "patients = patients.dropna('any', subset='doctor_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now count the missing values in `doctor_name` again. You should see output `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "patients.filter((patients[\"doctor_name\"] == \"\") | patients[\"doctor_name\"].isNull() | isnan(patients[\"doctor_name\"])).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the boolean columns `patient_show`, `is_regular_visist`, `patient_diabetic`, and `patient_allergic` to int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = patients.withColumn(\"patient_show\", patients.patient_show.cast('int'))\n",
    "patients = patients.withColumn(\"is_regular_visit\", patients.is_regular_visit.cast('int'))\n",
    "patients = patients.withColumn(\"patient_diabetic\", patients.patient_diabetic.cast('int'))\n",
    "patients = patients.withColumn(\"patient_allergic\", patients.patient_allergic.cast('int'))\n",
    "\n",
    "# patients.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a new column called `diagnosis_int`. The value in this column should be `0` if the corresponding row of the `diagnosis` column is `no diagnosis`. Otherwise the value should be `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|diagnosis_int|\n",
      "+-------------+\n",
      "|            1|\n",
      "|            0|\n",
      "|            0|\n",
      "|            0|\n",
      "|            0|\n",
      "|            0|\n",
      "|            1|\n",
      "|            0|\n",
      "|            1|\n",
      "|            0|\n",
      "|            1|\n",
      "|            1|\n",
      "|            1|\n",
      "|            0|\n",
      "|            0|\n",
      "|            0|\n",
      "|            1|\n",
      "|            0|\n",
      "|            0|\n",
      "|            0|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patients = patients.withColumn(\"diagnosis_int\", (patients['diagnosis']!='no diagnosis').cast('int'))\n",
    "\n",
    "patients.select('diagnosis_int').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a new column called `gender_int`. The value in this column should be `0` if the corresponding row of the `patient_gender` column is `Male`. Otherwise the value should be `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|gender_int|\n",
      "+----------+\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         0|\n",
      "|         1|\n",
      "|         1|\n",
      "|         0|\n",
      "|         1|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         1|\n",
      "|         0|\n",
      "|         0|\n",
      "|         1|\n",
      "|         0|\n",
      "|         1|\n",
      "|         1|\n",
      "|         0|\n",
      "|         1|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patients = patients.withColumn(\"gender_int\", (patients['patient_gender']=='Female').cast('int'))\n",
    "\n",
    "patients.select('gender_int').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the time limitation for this lab, we won't go through all steps of data wrangling like we did in the Unsupervised Learning with Scikit-Learn Lab. We'll simply keep the numerical columns we have so far and drop the rest so that we'll have time to practice MLlib.\n",
    "\n",
    "#### Drop the following columns from `patients`: `['id', 'patient_name', 'patient_email', 'patient_nhs_number', 'doctor_phone', 'patient_dob', 'doctor_name', 'appointment_date', 'prescribed_medicines', 'diagnosis', 'patient_gender']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "col_to_drop = ['id', 'patient_name', 'patient_email', 'patient_nhs_number', 'doctor_phone', 'patient_dob', 'doctor_name', 'appointment_date', 'prescribed_medicines', 'diagnosis', 'patient_gender']\n",
    "patients = patients.drop(*col_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call `printSchema()` of `patients` again. You should see all fields remaining are interger types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- patient_diabetic: integer (nullable = true)\n",
      " |-- patient_allergic: integer (nullable = true)\n",
      " |-- patient_weight_kg: integer (nullable = true)\n",
      " |-- patient_height_sm: integer (nullable = true)\n",
      " |-- patient_show: integer (nullable = true)\n",
      " |-- is_regular_visit: integer (nullable = true)\n",
      " |-- diagnosis_int: integer (nullable = false)\n",
      " |-- gender_int: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "patients.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You now get the feeling that Pyspark have similar features as Pandas to perform data wrangling. However data wrangling with Pyspark is not as easy as in Pandas. The real value of Pyspark is to analyze really big datasets with complex/massive data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans Analysis Example\n",
    "\n",
    "Finally we will show you an example of clustering the cleaned data with KMeans. Because you have done the same thing in the Supervised Learning with Scikit-Learn lab, we won't ask you to figure it out with Pyspark's MLlib. We are providing the code for you to learn.\n",
    "\n",
    "In order to perform KMeans analysis in Pyspark, **we need to use the `VectorAssembler` to convert the existing data into high-dimensional vectors** so that we can perform the KMeans clustering. **We will assign the generated vectors into a new column called `features`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|features                             |\n",
      "+-------------------------------------+\n",
      "|[0.0,1.0,59.0,176.0,1.0,1.0,1.0,1.0] |\n",
      "|[0.0,1.0,77.0,186.0,1.0,1.0,0.0,1.0] |\n",
      "|[1.0,1.0,90.0,177.0,0.0,0.0,0.0,1.0] |\n",
      "|[1.0,1.0,70.0,150.0,0.0,1.0,0.0,0.0] |\n",
      "|(8,[1,2,3,7],[1.0,82.0,140.0,1.0])   |\n",
      "|[0.0,1.0,105.0,179.0,0.0,1.0,0.0,1.0]|\n",
      "|[1.0,1.0,87.0,180.0,0.0,0.0,1.0,0.0] |\n",
      "|(8,[2,3,7],[73.0,152.0,1.0])         |\n",
      "|(8,[0,2,3,6],[1.0,83.0,156.0,1.0])   |\n",
      "|(8,[2,3,4],[89.0,160.0,1.0])         |\n",
      "|[1.0,1.0,108.0,165.0,0.0,1.0,1.0,0.0]|\n",
      "|[0.0,0.0,53.0,147.0,0.0,1.0,1.0,1.0] |\n",
      "|[1.0,0.0,51.0,170.0,1.0,1.0,1.0,0.0] |\n",
      "|[1.0,1.0,106.0,142.0,1.0,0.0,0.0,0.0]|\n",
      "|(8,[1,2,3,7],[1.0,58.0,169.0,1.0])   |\n",
      "|(8,[0,2,3],[1.0,64.0,150.0])         |\n",
      "|[1.0,0.0,56.0,168.0,1.0,1.0,1.0,1.0] |\n",
      "|[0.0,1.0,69.0,182.0,1.0,0.0,0.0,1.0] |\n",
      "|(8,[1,2,3,5],[1.0,69.0,178.0,1.0])   |\n",
      "|[0.0,1.0,77.0,155.0,1.0,1.0,0.0,1.0] |\n",
      "+-------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"patient_diabetic\", \"patient_allergic\", \"patient_weight_kg\", \"patient_height_sm\", \"patient_show\", \"is_regular_visit\", \"diagnosis_int\", \"gender_int\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(patients)\n",
    "output.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we import the `KMeans` class from Pyspark's MLlib and fit the new dataset with the `features` column to KMeans. We call `setK(4)` for `KMeans` to indicate we would like to receive 4 data clusters. Finally we print the center of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors = 119574.87827302358\n",
      "Cluster Centers: \n",
      "[  0.51260504   0.52941176  62.76890756 176.96638655   0.47058824\n",
      "   0.5          0.44957983   0.55042017]\n",
      "[  0.55504587   0.51834862  97.75229358 153.49541284   0.49082569\n",
      "   0.50458716   0.59174312   0.48165138]\n",
      "[  0.50643777   0.49785408  65.5751073  152.26180258   0.50643777\n",
      "   0.55364807   0.50643777   0.44635193]\n",
      "[  0.48616601   0.49407115  93.88142292 178.70355731   0.50988142\n",
      "   0.52964427   0.51778656   0.53754941]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans().setK(4).setSeed(1)\n",
    "\n",
    "model = kmeans.fit(output)\n",
    "\n",
    "wssse = model.computeCost(output)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
